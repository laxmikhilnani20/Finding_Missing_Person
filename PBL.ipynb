{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMOxxDPAtXrMWloUA6/9JI9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laxmikhilnani20/Finding_Missing_Person/blob/main/PBL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4GquNswai4i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ed7627ca",
        "outputId": "5af62f27-cf44-4318-d4b4-c92dcc3e08f6"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install facenet-pytorch torch torchvision opencv-python-headless scikit-learn pandas matplotlib pillow google-cloud-storage optuna\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from facenet-pytorch)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow\n",
            "  Downloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
            "Collecting torch\n",
            "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "INFO: pip is looking at multiple versions of opencv-python-headless to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.16.5)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Downloading facenet_pytorch-2.6.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, colorlog, opencv-python-headless, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, optuna, torchvision, facenet-pytorch\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorlog-6.9.0 facenet-pytorch-2.6.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 opencv-python-headless-4.11.0.86 optuna-4.5.0 pillow-10.2.0 torch-2.2.2 torchvision-0.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              },
              "id": "508776e2d08b43b39f32e02c2971c433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "Number of GPUs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "acefb1bf",
        "outputId": "1c538d27-765b-4517-a465-7f0e333b86e8"
      },
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Running on device: {device}')\n",
        "\n",
        "# Load MTCNN for face detection\n",
        "mtcnn = MTCNN(keep_all=True, thresholds=[0.5, 0.6, 0.7], device=device)\n",
        "print(\"MTCNN model loaded.\")\n",
        "\n",
        "# Load InceptionResnetV1 for face embeddings\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "print(\"InceptionResnetV1 model loaded.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'is_directory' from 'PIL._util' (/usr/local/lib/python3.12/dist-packages/PIL/_util.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3629356462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInceptionResnetV1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/facenet_pytorch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_resnet_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInceptionResnetV1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mONet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprewhiten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_image_standardization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/mtcnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/utils/detect_face.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatched_nms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_optical_flow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlyingChairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlyingThings3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHD1K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKittiFlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSintel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from ._stereo_matching import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mCarlaStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mCREStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mETH3DStereo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/_optical_flow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_png_16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_read_pfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_str_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_directory' from 'PIL._util' (/usr/local/lib/python3.12/dist-packages/PIL/_util.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "466f4978"
      },
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# 1. Upload a query image (query.jpg).\n",
        "uploaded = files.upload()\n",
        "query_image_name = list(uploaded.keys())[0]\n",
        "query_image = Image.open(query_image_name).convert('RGB')\n",
        "\n",
        "# 2. Use MTCNN to detect the face.\n",
        "# The 'mtcnn' object was loaded in a previous cell.\n",
        "boxes, _ = mtcnn.detect(query_image)\n",
        "\n",
        "if boxes is not None:\n",
        "    # Assume only one face for simplicity, take the first detected face\n",
        "    box = boxes[0]\n",
        "\n",
        "    # Crop the face from the image\n",
        "    face = query_image.crop(box)\n",
        "\n",
        "    # Preprocess the face for the resnet model\n",
        "    # Resize to 160x160 and convert to tensor\n",
        "    face_tensor = mtcnn.forward(query_image, save_path=None)[0].unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    # 3. Extract a 512-d embedding from the face using InceptionResnetV1.\n",
        "    # The 'resnet' object was loaded in a previous cell.\n",
        "    resnet.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        embedding = resnet(face_tensor)\n",
        "\n",
        "    # 4. Print the shape of the embedding vector.\n",
        "    print(f\"Shape of the embedding vector: {embedding.shape}\")\n",
        "\n",
        "else:\n",
        "    # 5. If no face is detected, print a warning.\n",
        "    print(\"Warning: No face detected in the query image.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IM6KuHoZg0ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825c9e30"
      },
      "source": [
        "# Task\n",
        "Write Colab code to process a video file (\"video1.mp4\"), extract frames at a fixed stride (e.g., every 10th frame), detect faces and extract embeddings from each frame, compute cosine similarity with a pre-computed query embedding, store the results (frame index, similarity score, match status based on a threshold of 0.7) in a pandas DataFrame, export the DataFrame to \"output_report.csv\", and print the first few rows of the DataFrame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed1648ca"
      },
      "source": [
        "## Upload video file\n",
        "\n",
        "### Subtask:\n",
        "Provide code to upload the video file to the Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13699b87"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary module and upload the video file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "035a24bf"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the video file\n",
        "uploaded_video = files.upload()\n",
        "\n",
        "# Get the name of the uploaded video file\n",
        "video_file_name = list(uploaded_video.keys())[0]\n",
        "print(f\"Uploaded video file: {video_file_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf8f5c1a"
      },
      "source": [
        "## Extract frames from video\n",
        "\n",
        "### Subtask:\n",
        "Write code to read the video and extract frames at a specified stride.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67c36f3"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to read the video and extract frames at a specified stride. The code will open the video file, iterate through frames, extract frames at the given stride, and store them in a list. Finally, it will print the number of extracted frames and release the video capture object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3f2da4b"
      },
      "source": [
        "import cv2\n",
        "\n",
        "# Define the video file name and the desired frame stride\n",
        "# video_file_name was obtained from the previous subtask\n",
        "frame_stride = 2 # Extract every 2nd frame\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "# Check if the video file was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    # Get video properties\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    print(f\"Total frames: {total_frames}\")\n",
        "    print(f\"Frames per second (fps): {fps}\")\n",
        "\n",
        "    # Initialize list to store extracted frames\n",
        "    extracted_frames = []\n",
        "    frame_index = 0\n",
        "\n",
        "    # Loop through the video frames\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Break the loop if no frame is read\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Check if the current frame index is a multiple of the desired stride\n",
        "        if frame_index % frame_stride == 0:\n",
        "            # Convert BGR to RGB as PIL Image expects RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            extracted_frames.append((frame_index, rgb_frame))\n",
        "\n",
        "        frame_index += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Print the total number of frames extracted\n",
        "    print(f\"Total frames extracted: {len(extracted_frames)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a986cc03"
      },
      "source": [
        "## Process each frame\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the extracted frames and for each frame:\n",
        "- Detect faces using the loaded MTCNN model.\n",
        "- If a face is detected, extract its embedding using the loaded InceptionResnetV1 model.\n",
        "- Compute the cosine similarity between the extracted face embedding and the query embedding.\n",
        "- Determine if there is a match based on a similarity threshold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20df38c7"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to iterate through extracted frames, detect faces, extract embeddings, compute similarity, and determine match status, storing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75b8687c"
      },
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "# Define the similarity threshold\n",
        "similarity_threshold = 0.7\n",
        "\n",
        "# Initialize list to store results\n",
        "results = []\n",
        "\n",
        "# Ensure query_embedding is a tensor and on the correct device\n",
        "# Assuming query_embedding was defined in a previous cell and is a torch tensor\n",
        "# If it's not, you'll need to load or generate it here.\n",
        "# For this example, let's assume query_embedding is already a tensor of shape [1, 512]\n",
        "# on the correct device (e.g., 'cuda:0')\n",
        "# print(f\"Query embedding shape: {query_embedding.shape}\") # Uncomment to verify\n",
        "\n",
        "# Loop through the extracted frames\n",
        "for original_frame_index, frame in extracted_frames:\n",
        "    # Convert OpenCV frame (NumPy array) to PIL Image\n",
        "    pil_image = Image.fromarray(frame) # Frame is already in RGB\n",
        "\n",
        "    # Detect faces using MTCNN\n",
        "    boxes, _ = mtcnn.detect(pil_image)\n",
        "\n",
        "    highest_similarity = None\n",
        "    match_found = False\n",
        "    best_match_bbox = [None, None, None, None] # Initialize bounding box coordinates\n",
        "\n",
        "    if boxes is not None:\n",
        "        # Extract embeddings for all detected faces\n",
        "        face_tensors = mtcnn.forward(pil_image, save_path=None)\n",
        "\n",
        "        if face_tensors is not None:\n",
        "            face_tensors = face_tensors.to(device)\n",
        "\n",
        "            # Extract embeddings using InceptionResnetV1\n",
        "            resnet.eval() # Set the model to evaluation mode\n",
        "            with torch.no_grad(): # Disable gradient calculation\n",
        "                face_embeddings = resnet(face_tensors)\n",
        "\n",
        "            # Compute cosine similarity with the query embedding for each detected face\n",
        "            # Ensure both tensors have the same dimensions for cosine similarity\n",
        "            # Reshape query_embedding to match face_embeddings for batch cosine similarity\n",
        "            query_embedding_batch = embedding.repeat(face_embeddings.shape[0], 1)\n",
        "            similarities = F.cosine_similarity(face_embeddings, query_embedding_batch)\n",
        "\n",
        "            # Find the highest similarity score and the index of the best match\n",
        "            highest_similarity, best_match_index = torch.max(similarities, dim=0)\n",
        "            highest_similarity = highest_similarity.item()\n",
        "\n",
        "            # Determine match status based on the highest similarity\n",
        "            match_found = highest_similarity >= similarity_threshold\n",
        "\n",
        "            # If a match is found, store the bounding box of the best match\n",
        "            if match_found:\n",
        "                best_match_bbox = [int(b) for b in boxes[best_match_index.item()]]\n",
        "\n",
        "\n",
        "    # Append results\n",
        "    results.append({\n",
        "        'frame_index': original_frame_index,\n",
        "        'similarity': highest_similarity,\n",
        "        'match_status': match_found,\n",
        "        'bbox_x1': best_match_bbox[0],\n",
        "        'bbox_y1': best_match_bbox[1],\n",
        "        'bbox_x2': best_match_bbox[2],\n",
        "        'bbox_y2': best_match_bbox[3]\n",
        "    })\n",
        "\n",
        "print(f\"Processed {len(results)} frames and stored results.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc784df6"
      },
      "source": [
        "## Store results in dataframe\n",
        "\n",
        "### Subtask:\n",
        "Collect the frame index, similarity score, and match status for each processed frame into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f1a929a"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a pandas DataFrame from the collected results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae2369a8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a pandas DataFrame from the results list\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"Results DataFrame:\")\n",
        "display(results_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1891e14"
      },
      "source": [
        "## Export results to CSV\n",
        "\n",
        "### Subtask:\n",
        "Save the DataFrame to a CSV file named \"output_report.csv\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92953321"
      },
      "source": [
        "**Reasoning**:\n",
        "Export the DataFrame to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3226ac16"
      },
      "source": [
        "# Export the DataFrame to a CSV file\n",
        "output_csv_filename = \"output_report.csv\"\n",
        "results_df.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "print(f\"Results exported to {output_csv_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba23c1b8"
      },
      "source": [
        "# Task\n",
        "Write Colab code to:\n",
        "1. Load the \"output_report.csv\".\n",
        "2. Filter rows where match_status == True.\n",
        "3. Print how many frames matched.\n",
        "4. For the first 5 matched frames:\n",
        "   - Reload that frame from the video.\n",
        "   - Detect the face(s) in the frame.\n",
        "   - Draw bounding box(es) around the detected face(s).\n",
        "   - Show the frame using matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "271ebf93"
      },
      "source": [
        "## Load the report\n",
        "\n",
        "### Subtask:\n",
        "Load the \"output_report.csv\" file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d22915fe"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the \"output_report.csv\" file into a pandas DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "488e6960"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "results_df = pd.read_csv(\"output_report.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(\"Loaded results DataFrame:\")\n",
        "display(results_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f156b020"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the DataFrame to include only rows where 'match_status' is True, count the number of matching frames, and print the count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ac4547"
      },
      "source": [
        "# Filter rows where match_status == True\n",
        "matched_frames_df = results_df[results_df['match_status'] == True]\n",
        "\n",
        "# Print how many frames matched\n",
        "num_matched_frames = len(matched_frames_df)\n",
        "print(f\"Number of matched frames: {num_matched_frames}\")\n",
        "\n",
        "# Display the first few rows of the filtered DataFrame\n",
        "print(\"Matched frames DataFrame:\")\n",
        "display(matched_frames_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5afc92f9"
      },
      "source": [
        "**Reasoning**:\n",
        "For the first 5 matched frames, reload the frame from the video, detect faces, draw bounding boxes, and display the frame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b4b5549"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure video_file_name is defined (from previous cells)\n",
        "# Ensure mtcnn and resnet are loaded (from previous cells)\n",
        "# Ensure device is set (from previous cells)\n",
        "\n",
        "# Open the video file again\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    # Get the frame indices and bounding box coordinates of the first 5 matched frames\n",
        "    frames_to_process = matched_frames_df[['frame_index', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].to_dict('records')[:5]\n",
        "\n",
        "    print(f\"Processing first {len(frames_to_process)} matched frames:\")\n",
        "\n",
        "    for frame_info in frames_to_process:\n",
        "        frame_index = frame_info['frame_index']\n",
        "        bbox = [frame_info['bbox_x1'], frame_info['bbox_y1'], frame_info['bbox_x2'], frame_info['bbox_y2']]\n",
        "\n",
        "        # Ensure bounding box coordinates are not None\n",
        "        if None in bbox:\n",
        "            print(f\"Skipping frame {frame_index} due to missing bounding box coordinates.\")\n",
        "            continue\n",
        "\n",
        "        # Seek to the specific frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "\n",
        "        # Read the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            # Convert BGR to RGB for displaying with matplotlib\n",
        "            frame_to_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Draw bounding box using coordinates from the DataFrame\n",
        "            # Ensure coordinates are integers\n",
        "            bbox = [int(b) for b in bbox]\n",
        "            cv2.rectangle(frame_to_display, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2) # Green box\n",
        "\n",
        "\n",
        "            # Show the frame using matplotlib\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(frame_to_display)\n",
        "            plt.title(f\"Frame {frame_index} with matched face\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: Could not read frame {frame_index}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5b6c9c9"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   11 frames out of the total in the report had a `match_status` of True.\n",
        "*   The first 5 matched frames were successfully processed, including reloading the frame from the video, detecting faces, drawing bounding boxes, and displaying the frames.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Analyze the characteristics of the matched frames to understand why these specific frames resulted in a positive match status.\n",
        "*   Extend the processing to a larger sample of matched frames to gain broader insights into the face detection and matching performance across the video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1eb66fd"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-export the `results_df` (which now contains bounding box coordinates) to the CSV file so that the visualization code can access them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fad9344"
      },
      "source": [
        "# Export the DataFrame with bounding box coordinates to a CSV file\n",
        "output_csv_filename = \"output_report.csv\"\n",
        "results_df.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "print(f\"Updated results exported to {output_csv_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ff52ea6"
      },
      "source": [
        "**Reasoning**:\n",
        "Reload the CSV file into a pandas DataFrame to ensure the DataFrame used for filtering contains the bounding box information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8d9defc"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "results_df = pd.read_csv(\"output_report.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(\"Reloaded results DataFrame:\")\n",
        "display(results_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d251bd34"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the DataFrame to include only rows where 'match_status' is True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e947267e"
      },
      "source": [
        "# Filter rows where match_status == True\n",
        "matched_frames_df = results_df[results_df['match_status'] == True]\n",
        "\n",
        "# Print how many frames matched\n",
        "num_matched_frames = len(matched_frames_df)\n",
        "print(f\"Number of matched frames: {num_matched_frames}\")\n",
        "\n",
        "# Display the first few rows of the filtered DataFrame\n",
        "print(\"Matched frames DataFrame (with bbox columns):\")\n",
        "display(matched_frames_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "610d14f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `matched_frames_df` contains the bounding box coordinates, retry visualizing the first few matched frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38822b77"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure video_file_name is defined (from previous cells)\n",
        "# Ensure mtcnn and resnet are loaded (from previous cells)\n",
        "# Ensure device is set (from previous cells)\n",
        "\n",
        "# Open the video file again\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    # Get the frame indices and bounding box coordinates of the first 5 matched frames\n",
        "    frames_to_process = matched_frames_df[['frame_index', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].to_dict('records')[:5]\n",
        "\n",
        "    print(f\"Processing first {len(frames_to_process)} matched frames:\")\n",
        "\n",
        "    for frame_info in frames_to_process:\n",
        "        frame_index = frame_info['frame_index']\n",
        "        bbox = [frame_info['bbox_x1'], frame_info['bbox_y1'], frame_info['bbox_x2'], frame_info['bbox_y2']]\n",
        "\n",
        "        # Ensure bounding box coordinates are not None\n",
        "        if None in bbox:\n",
        "            print(f\"Skipping frame {frame_index} due to missing bounding box coordinates.\")\n",
        "            continue\n",
        "\n",
        "        # Seek to the specific frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "\n",
        "        # Read the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            # Convert BGR to RGB for displaying with matplotlib\n",
        "            frame_to_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Draw bounding box using coordinates from the DataFrame\n",
        "            # Ensure coordinates are integers\n",
        "            bbox = [int(b) for b in bbox]\n",
        "            cv2.rectangle(frame_to_display, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2) # Green box\n",
        "\n",
        "\n",
        "            # Show the frame using matplotlib\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(frame_to_display)\n",
        "            plt.title(f\"Frame {frame_index} with matched face\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: Could not read frame {frame_index}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7471ad46"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* 11 frames out of the total in the report had a `match_status` of True.\n",
        "* The first 5 matched frames were successfully processed, including reloading the frame from the video, detecting faces, drawing bounding boxes, and displaying the frames.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* Analyze the characteristics of the matched frames to understand why these specific frames resulted in a positive match status.\n",
        "* Extend the processing to a larger sample of matched frames to gain broader insights into the face detection and matching performance across the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03448eae"
      },
      "source": [
        "# Task\n",
        "Write Colab code to:\n",
        "1. Load the \"output_report.csv\".\n",
        "2. Filter rows where match == False.\n",
        "3. Randomly select 5 frames from this set.\n",
        "4. For each selected frame:\n",
        "   - Reload the frame from the video.\n",
        "   - If any faces were detected (i.e., bbox coordinates are not None), draw their bounding boxes in red.\n",
        "   - Add a title \"No Match Detected\".\n",
        "   - Show the frame using matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e3f1b44"
      },
      "source": [
        "## Load the report\n",
        "\n",
        "### Subtask:\n",
        "Load the \"output_report.csv\" file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76517cfe"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the \"output_report.csv\" file into a pandas DataFrame and display the head.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c34b64e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "results_df = pd.read_csv(\"output_report.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(\"Loaded results DataFrame:\")\n",
        "display(results_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960514ba"
      },
      "source": [
        "## Filter non-matched frames\n",
        "\n",
        "### Subtask:\n",
        "Filter the DataFrame to keep only the rows where `match_status` is False.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b0b11b5"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the DataFrame to include only rows where 'match_status' is False, count the number of non-matching frames, and print the count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6be5ab"
      },
      "source": [
        "# Filter rows where match_status == False\n",
        "non_matched_frames_df = results_df[results_df['match_status'] == False]\n",
        "\n",
        "# Print how many frames did not match\n",
        "num_non_matched_frames = len(non_matched_frames_df)\n",
        "print(f\"Number of non-matched frames: {num_non_matched_frames}\")\n",
        "\n",
        "# Display the first few rows of the filtered DataFrame\n",
        "print(\"Non-matched frames DataFrame:\")\n",
        "display(non_matched_frames_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2390644"
      },
      "source": [
        "## Randomly select frames\n",
        "\n",
        "### Subtask:\n",
        "Randomly select 5 rows from the filtered DataFrame (`non_matched_frames_df`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba648267"
      },
      "source": [
        "**Reasoning**:\n",
        "Randomly select 5 rows from the `non_matched_frames_df` for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d271f8a"
      },
      "source": [
        "# Randomly select 5 frames from the non-matched frames DataFrame\n",
        "# Use .sample() to randomly select rows\n",
        "# Use .reset_index(drop=True) to reset the index of the sampled DataFrame\n",
        "selected_non_matched_frames = non_matched_frames_df.sample(n=min(5, len(non_matched_frames_df)), random_state=42).reset_index(drop=True) # Use a random state for reproducibility\n",
        "\n",
        "print(f\"Randomly selected {len(selected_non_matched_frames)} non-matched frames:\")\n",
        "display(selected_non_matched_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2fc87c9"
      },
      "source": [
        "## Load and process selected frames\n",
        "\n",
        "### Subtask:\n",
        "For each selected non-matched frame, reload it from the video, draw red bounding boxes if faces were detected, add a title, and display it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7a97520"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the randomly selected non-matched frames, reload each frame from the video, check if bounding box coordinates exist (indicating a face was detected), draw red bounding boxes if they exist, add a title, and display the frame using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933ea3dd"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure video_file_name is defined (from previous cells)\n",
        "# Ensure mtcnn and resnet are loaded (from previous cells)\n",
        "# Ensure device is set (from previous cells)\n",
        "\n",
        "# Open the video file again\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    print(f\"Processing {len(selected_non_matched_frames)} randomly selected non-matched frames:\")\n",
        "\n",
        "    for index, frame_info in selected_non_matched_frames.iterrows():\n",
        "        frame_index = frame_info['frame_index']\n",
        "        bbox = [frame_info['bbox_x1'], frame_info['bbox_y1'], frame_info['bbox_x2'], frame_info['bbox_y2']]\n",
        "\n",
        "        # Seek to the specific frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "\n",
        "        # Read the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            # Convert BGR to RGB for displaying with matplotlib\n",
        "            frame_to_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Check if bounding box coordinates are available (face was detected)\n",
        "            if not pd.isna(bbox[0]):\n",
        "                # Draw bounding box using coordinates from the DataFrame in red\n",
        "                # Ensure coordinates are integers\n",
        "                bbox = [int(b) for b in bbox]\n",
        "                cv2.rectangle(frame_to_display, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2) # Red box\n",
        "\n",
        "            # Show the frame using matplotlib\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(frame_to_display)\n",
        "            plt.title(f\"Frame {frame_index} - No Match Detected\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: Could not read frame {frame_index}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40f7fb0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* We filtered for frames where no match was detected based on the similarity threshold.\n",
        "* We randomly selected 5 of these non-matched frames.\n",
        "* For each selected non-matched frame, we reloaded it from the video and visualized it.\n",
        "* Red bounding boxes were drawn around detected faces in these frames, even though their embeddings did not meet the similarity threshold for a match.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* Review the visualized non-matched frames with detected faces to understand why they did not match the query embedding. This could be due to variations in pose, lighting, or occlusion.\n",
        "* Consider adjusting the similarity threshold or exploring different face recognition models to improve performance if needed.\n",
        "* You can now proceed with other tasks or further analysis based on the generated `output_report.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ffa1dec"
      },
      "source": [
        "**Finish task**: The visualization\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "of randomly selected non-matched frames is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59618557"
      },
      "source": [
        "# Task\n",
        "Write Colab code to process a video file (\"video1.mp4\"), extract frames with a stride of 2, detect all faces in each frame using MTCNN (keep_all=True, thresholds=[0.5, 0.6, 0.7]), extract embeddings for each detected face using InceptionResnetV1, compare each face embedding with a pre-computed query embedding using cosine similarity, mark a frame as a match if ANY face in the frame has a similarity score >= 0.6, save the details (frame index, face ID, similarity score, match status, and bounding box coordinates) for ALL faces that meet the similarity threshold in a pandas DataFrame with columns [\"frame_index\", \"face_id\", \"similarity_score\", \"match\", \"bbox_x1\", \"bbox_y1\", \"bbox_x2\", \"bbox_y2\"], export the DataFrame to \"output_report.csv\", and print the first few rows of the DataFrame. Assume \"video1.mp4\" and the query embedding are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0218dbb7"
      },
      "source": [
        "## Modify frame processing logic\n",
        "\n",
        "### Subtask:\n",
        "Update the code that iterates through frames to detect all faces, extract embeddings for each face, compute cosine similarity with the query embedding, identify faces meeting the similarity threshold (>= 0.6), and mark the frame as a match if any face meets the threshold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa2d1c42"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the extracted frames, detect all faces, extract embeddings for each face, compute similarity with the query embedding, identify faces meeting the similarity threshold, and store the details for those faces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59910135"
      },
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Define the similarity threshold (updated to 0.6 as per the task)\n",
        "similarity_threshold = 0.6\n",
        "\n",
        "# Initialize list to store results for individual faces that match\n",
        "results = []\n",
        "\n",
        "# Ensure query_embedding is a tensor and on the correct device\n",
        "# Assuming query_embedding was defined in a previous cell and is a torch tensor\n",
        "# If it's not, you'll need to load or generate it here.\n",
        "# For this example, let's assume query_embedding is already a tensor of shape [1, 512]\n",
        "# on the correct device (e.g., 'cuda:0')\n",
        "# print(f\"Query embedding shape: {query_embedding.shape}\") # Uncomment to verify\n",
        "\n",
        "face_counter = 0 # Initialize a counter for unique face IDs\n",
        "\n",
        "# Loop through the extracted frames\n",
        "for original_frame_index, frame in extracted_frames:\n",
        "    # Convert OpenCV frame (NumPy array) to PIL Image\n",
        "    pil_image = Image.fromarray(frame) # Frame is already in RGB\n",
        "\n",
        "    # Detect faces using MTCNN with keep_all=True\n",
        "    boxes, probs = mtcnn.detect(pil_image, keep_all=True)\n",
        "\n",
        "    faces_in_frame_results = [] # Store results for faces that meet threshold in the current frame\n",
        "\n",
        "    if boxes is not None:\n",
        "        # Extract embeddings for all detected faces\n",
        "        face_tensors = mtcnn.forward(pil_image, save_path=None)\n",
        "\n",
        "        if face_tensors is not None:\n",
        "            face_tensors = face_tensors.to(device)\n",
        "\n",
        "            # Extract embeddings using InceptionResnetV1\n",
        "            resnet.eval() # Set the model to evaluation mode\n",
        "            with torch.no_grad(): # Disable gradient calculation\n",
        "                face_embeddings = resnet(face_tensors)\n",
        "\n",
        "            # Compute cosine similarity with the query embedding for each detected face\n",
        "            # Ensure both tensors have the same dimensions for cosine similarity\n",
        "            # Reshape query_embedding to match face_embeddings for batch cosine similarity\n",
        "            query_embedding_batch = embedding.repeat(face_embeddings.shape[0], 1)\n",
        "            similarities = F.cosine_similarity(face_embeddings, query_embedding_batch)\n",
        "\n",
        "            # Iterate through detected faces and their similarities\n",
        "            for i in range(len(boxes)):\n",
        "                similarity = similarities[i].item()\n",
        "                box = boxes[i]\n",
        "\n",
        "                # Check if the similarity meets the threshold\n",
        "                if similarity >= similarity_threshold:\n",
        "                    face_counter += 1 # Increment face counter for a unique ID\n",
        "\n",
        "                    # Append details for this matching face\n",
        "                    faces_in_frame_results.append({\n",
        "                        'frame_index': original_frame_index,\n",
        "                        'face_id': face_counter,\n",
        "                        'similarity_score': similarity,\n",
        "                        'match': True,\n",
        "                        'bbox_x1': int(box[0]),\n",
        "                        'bbox_y1': int(box[1]),\n",
        "                        'bbox_x2': int(box[2]),\n",
        "                        'bbox_y2': int(box[3])\n",
        "                    })\n",
        "\n",
        "    # If any face in the frame met the similarity threshold, extend the main results list\n",
        "    if faces_in_frame_results:\n",
        "        results.extend(faces_in_frame_results)\n",
        "\n",
        "# Print the total number of individual faces (across all frames) that met the threshold\n",
        "print(f\"Processed {len(extracted_frames)} frames.\")\n",
        "print(f\"Total individual faces with similarity >= {similarity_threshold}: {len(results)}\")\n",
        "\n",
        "# Create a pandas DataFrame from the results list\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"\\nResults DataFrame (first 5 rows):\")\n",
        "if not results_df.empty:\n",
        "    display(results_df.head())\n",
        "else:\n",
        "    print(\"No faces met the similarity threshold in any frame.\")\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "output_csv_filename = \"output_report.csv\"\n",
        "results_df.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "print(f\"\\nResults exported to {output_csv_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344e356a"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `detect` method of the `MTCNN` object does not accept the `keep_all` argument. The `keep_all` argument should be set during the initialization of the `MTCNN` object. The previous code already initialized `mtcnn` with `keep_all=True`. Therefore, I will remove the `keep_all=True` from the `detect` method call.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpHIpyx5rrkl"
      },
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Define the similarity threshold (updated to 0.6 as per the task)\n",
        "similarity_threshold = 0.6\n",
        "\n",
        "# Initialize list to store results for individual faces that match\n",
        "results = []\n",
        "\n",
        "# Ensure query_embedding is a tensor and on the correct device\n",
        "# Assuming query_embedding was defined in a previous cell and is a torch tensor\n",
        "# If it's not, you'll need to load or generate it here.\n",
        "# For this example, let's assume query_embedding is already a tensor of shape [1, 512]\n",
        "# on the correct device (e.g., 'cuda:0')\n",
        "# print(f\"Query embedding shape: {query_embedding.shape}\") # Uncomment to verify\n",
        "\n",
        "face_counter = 0 # Initialize a counter for unique face IDs\n",
        "\n",
        "# Loop through the extracted frames\n",
        "for original_frame_index, frame in extracted_frames:\n",
        "    # Convert OpenCV frame (NumPy array) to PIL Image\n",
        "    pil_image = Image.fromarray(frame) # Frame is already in RGB\n",
        "\n",
        "    # Detect faces using MTCNN\n",
        "    # keep_all=True is set during MTCNN initialization, so no need to pass it here\n",
        "    boxes, probs = mtcnn.detect(pil_image)\n",
        "\n",
        "    faces_in_frame_results = [] # Store results for faces that meet threshold in the current frame\n",
        "\n",
        "    if boxes is not None:\n",
        "        # Extract embeddings for all detected faces\n",
        "        face_tensors = mtcnn.forward(pil_image, save_path=None)\n",
        "\n",
        "        if face_tensors is not None:\n",
        "            face_tensors = face_tensors.to(device)\n",
        "\n",
        "            # Extract embeddings using InceptionResnetV1\n",
        "            resnet.eval() # Set the model to evaluation mode\n",
        "            with torch.no_grad(): # Disable gradient calculation\n",
        "                face_embeddings = resnet(face_tensors)\n",
        "\n",
        "            # Compute cosine similarity with the query embedding for each detected face\n",
        "            # Ensure both tensors have the same dimensions for cosine similarity\n",
        "            # Reshape query_embedding to match face_embeddings for batch cosine similarity\n",
        "            query_embedding_batch = embedding.repeat(face_embeddings.shape[0], 1)\n",
        "            similarities = F.cosine_similarity(face_embeddings, query_embedding_batch)\n",
        "\n",
        "            # Iterate through detected faces and their similarities\n",
        "            for i in range(len(boxes)):\n",
        "                similarity = similarities[i].item()\n",
        "                box = boxes[i]\n",
        "\n",
        "                # Check if the similarity meets the threshold\n",
        "                if similarity >= similarity_threshold:\n",
        "                    face_counter += 1 # Increment face counter for a unique ID\n",
        "\n",
        "                    # Append details for this matching face\n",
        "                    faces_in_frame_results.append({\n",
        "                        'frame_index': original_frame_index,\n",
        "                        'face_id': face_counter,\n",
        "                        'similarity_score': similarity,\n",
        "                        'match': True,\n",
        "                        'bbox_x1': int(box[0]),\n",
        "                        'bbox_y1': int(box[1]),\n",
        "                        'bbox_x2': int(box[2]),\n",
        "                        'bbox_y2': int(box[3])\n",
        "                    })\n",
        "\n",
        "    # If any face in the frame met the similarity threshold, extend the main results list\n",
        "    if faces_in_frame_results:\n",
        "        results.extend(faces_in_frame_results)\n",
        "\n",
        "# Print the total number of individual faces (across all frames) that met the threshold\n",
        "print(f\"Processed {len(extracted_frames)} frames.\")\n",
        "print(f\"Total individual faces with similarity >= {similarity_threshold}: {len(results)}\")\n",
        "\n",
        "# Create a pandas DataFrame from the results list\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"\\nResults DataFrame (first 5 rows):\")\n",
        "if not results_df.empty:\n",
        "    display(results_df.head())\n",
        "else:\n",
        "    print(\"No faces met the similarity threshold in any frame.\")\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "output_csv_filename = \"output_report.csv\"\n",
        "results_df.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "print(f\"\\nResults exported to {output_csv_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99d63d60"
      },
      "source": [
        "## Create dataframe with updated columns\n",
        "\n",
        "### Subtask:\n",
        "Create a pandas DataFrame from the list of results with the specified columns: [\"frame_index\", \"face_id\", \"similarity_score\", \"match\", \"bbox_x1\", \"bbox_y1\", \"bbox_x2\", \"bbox_y2\"].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e6b19a6"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a pandas DataFrame from the collected results list with the specified columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ffab6c2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a pandas DataFrame from the results list\n",
        "results_df = pd.DataFrame(results, columns=[\"frame_index\", \"face_id\", \"similarity_score\", \"match\", \"bbox_x1\", \"bbox_y1\", \"bbox_x2\", \"bbox_y2\"])\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(\"Results DataFrame:\")\n",
        "display(results_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5c8ffab"
      },
      "source": [
        "## Export updated results to csv\n",
        "\n",
        "### Subtask:\n",
        "Save the updated DataFrame to \"output_report.csv\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465f3eff"
      },
      "source": [
        "**Reasoning**:\n",
        "Export the DataFrame to a CSV file and print a confirmation message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34c8b379"
      },
      "source": [
        "# Export the DataFrame to a CSV file\n",
        "output_csv_filename = \"output_report.csv\"\n",
        "results_df.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "print(f\"Results exported to {output_csv_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3947db6a"
      },
      "source": [
        "## Display updated results\n",
        "\n",
        "### Subtask:\n",
        "Print the first few rows of the updated DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75f83d08"
      },
      "source": [
        "**Reasoning**:\n",
        "Print a message and display the first few rows of the updated DataFrame to verify its content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d4536f4"
      },
      "source": [
        "# Print a message indicating that you are displaying the first few rows\n",
        "print(\"First few rows of the updated DataFrame:\")\n",
        "\n",
        "# Display the head of the results_df DataFrame\n",
        "display(results_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1f14eb4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The analysis successfully processed frames extracted from the video \"video1.mp4\" with a stride of 2.\n",
        "*   MTCNN was used to detect all faces in each extracted frame (`keep_all=True`).\n",
        "*   InceptionResnetV1 was used to extract embeddings for each detected face.\n",
        "*   Cosine similarity was computed between each face embedding and a pre-computed query embedding.\n",
        "*   Faces with a cosine similarity score of 0.6 or higher were identified as matches.\n",
        "*   A unique `face_id` was assigned to each individual face that met the similarity threshold across all frames.\n",
        "*   Details for all faces meeting the similarity threshold (>= 0.6), including `frame_index`, `face_id`, `similarity_score`, `match` status (always True for the collected results), and bounding box coordinates (`bbox_x1`, `bbox_y1`, `bbox_x2`, `bbox_y2`), were stored in a pandas DataFrame.\n",
        "*   The DataFrame containing the results was successfully exported to \"output_report.csv\".\n",
        "*   The first few rows of the resulting DataFrame were printed for verification.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current output file \"output_report.csv\" only contains details for faces that met the similarity threshold. Consider modifying the process to include all detected faces in the output, adding a column to indicate whether they were a match or not based on the threshold.\n",
        "*   Visualize the results by drawing bounding boxes and similarity scores on the video frames where matches were found, potentially highlighting the specific faces that met the threshold.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81548c38"
      },
      "source": [
        "# Task\n",
        "Write Colab code to:\n",
        "1. Reload the processed video (\"video1.mp4\").\n",
        "2. Use MTCNN (keep_all=True) to detect all faces in every frame at a stride of 2.\n",
        "3. For each frame, store:\n",
        "   - frame_index\n",
        "   - number_of_faces_detected\n",
        "   - bounding box coordinates for each detected face\n",
        "4. Save this information in a CSV file \"all_faces_report.csv\".\n",
        "5. For visualization:\n",
        "   - Randomly select 10 frames where at least one face was detected.\n",
        "   - Draw bounding boxes around all detected faces (use red for non-matched, green if the face matched the query embedding).\n",
        "   - Display these frames using matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b2e9fb"
      },
      "source": [
        "## Process video and detect all faces\n",
        "\n",
        "### Subtask:\n",
        "Reload the video file, iterate through frames at a stride of 2, detect all faces in each frame using MTCNN, and store the frame index, number of detected faces, and bounding box coordinates for each face.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "749984f4"
      },
      "source": [
        "**Reasoning**:\n",
        "Reload the video file, iterate through frames at a stride of 2, detect all faces using MTCNN, and store the frame index, face ID, and bounding box coordinates for each detected face in a list of dictionaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "166d18bf"
      },
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Open the video file again\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    # Initialize list to store results for all detected faces\n",
        "    all_faces_data = []\n",
        "\n",
        "    face_counter = 0 # Initialize a counter for unique face IDs\n",
        "\n",
        "    frame_index = 0\n",
        "    # Loop through the video frames\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Break the loop if no frame is read\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Check if the current frame index is a multiple of the desired stride\n",
        "        if frame_index % frame_stride == 0:\n",
        "            # Convert BGR to RGB as PIL Image expects RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            pil_image = Image.fromarray(rgb_frame)\n",
        "\n",
        "            # Detect faces using MTCNN (keep_all=True is set during initialization)\n",
        "            boxes, probs = mtcnn.detect(pil_image)\n",
        "\n",
        "            if boxes is not None:\n",
        "                # Iterate through detected faces and their bounding boxes\n",
        "                for i in range(len(boxes)):\n",
        "                    box = boxes[i]\n",
        "                    face_counter += 1 # Increment face counter for a unique ID\n",
        "\n",
        "                    # Append details for this detected face\n",
        "                    all_faces_data.append({\n",
        "                        'frame_index': frame_index,\n",
        "                        'face_id': face_counter,\n",
        "                        'bbox_x1': int(box[0]),\n",
        "                        'bbox_y1': int(box[1]),\n",
        "                        'bbox_x2': int(box[2]),\n",
        "                        'bbox_y2': int(box[3])\n",
        "                    })\n",
        "\n",
        "        frame_index += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Print the total number of records collected\n",
        "    print(f\"Total records collected for all detected faces: {len(all_faces_data)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddff131"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask of reloading the video, iterating through frames, detecting faces, and collecting the data into `all_faces_data` is complete. Now, I need to create a pandas DataFrame from this list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cba2e7c0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a pandas DataFrame from the all_faces_data list\n",
        "all_faces_df = pd.DataFrame(all_faces_data)\n",
        "\n",
        "# Print the number of faces detected per frame\n",
        "faces_per_frame = all_faces_df.groupby('frame_index').size().reset_index(name='number_of_faces_detected')\n",
        "print(\"\\nNumber of faces detected per frame (first 5 rows):\")\n",
        "display(faces_per_frame.head())\n",
        "\n",
        "# Display the first few rows of the all_faces_df DataFrame\n",
        "print(\"\\nAll faces DataFrame (first 5 rows):\")\n",
        "display(all_faces_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17ad0bc"
      },
      "source": [
        "## Create and export \"all faces report.csv\"\n",
        "\n",
        "### Subtask:\n",
        "Convert the list of results into a pandas DataFrame with appropriate columns (e.g., 'frame_index', 'face_id', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2'). Assign a unique `face_id` to each detected face across all frames. Export this DataFrame to a new CSV file named \"all_faces_report.csv\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d1430cb"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a pandas DataFrame from the collected results list with the specified columns, export it to a CSV file, and print a confirmation message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eebfcea"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a pandas DataFrame from the all_faces_data list\n",
        "all_faces_df = pd.DataFrame(all_faces_data, columns=[\"frame_index\", \"face_id\", \"bbox_x1\", \"bbox_y1\", \"bbox_x2\", \"bbox_y2\"])\n",
        "\n",
        "# Export the all_faces_df DataFrame to a CSV file\n",
        "output_csv_filename = \"all_faces_report.csv\"\n",
        "all_faces_df.to_csv(output_csv_filename, index=False)\n",
        "\n",
        "# Print a confirmation message\n",
        "print(f\"All detected faces report exported to {output_csv_filename}\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "print(\"\\nAll faces DataFrame (first 5 rows):\")\n",
        "display(all_faces_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed769440"
      },
      "source": [
        "## Load and merge reports\n",
        "\n",
        "### Subtask:\n",
        "Load \"all_faces_report.csv\" and \"output_report.csv\" into pandas DataFrames. Merge these two DataFrames to associate match status with all detected faces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb598dca"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the two CSV files into pandas DataFrames and merge them based on frame index and bounding box coordinates to associate match status with all detected faces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "567a6a56"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files into pandas DataFrames\n",
        "all_faces_df = pd.read_csv(\"all_faces_report.csv\")\n",
        "matched_faces_df = pd.read_csv(\"output_report.csv\")\n",
        "\n",
        "# Convert bounding box coordinates to integers to handle potential floating-point inaccuracies before merging\n",
        "all_faces_df[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']] = all_faces_df[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].astype(int)\n",
        "matched_faces_df[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']] = matched_faces_df[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].astype(int)\n",
        "\n",
        "\n",
        "# Merge the two DataFrames\n",
        "# Use a left merge with all_faces_df to keep all detected faces\n",
        "merged_df = pd.merge(\n",
        "    all_faces_df,\n",
        "    matched_faces_df[['frame_index', 'similarity_score', 'match', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']],\n",
        "    on=['frame_index', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Fill NaN values in the 'match' column with False\n",
        "merged_df['match'] = merged_df['match'].fillna(False)\n",
        "\n",
        "# Select and reorder columns\n",
        "merged_df = merged_df[['frame_index', 'face_id', 'similarity_score', 'match', 'bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']]\n",
        "\n",
        "# Display the first few rows of the merged DataFrame\n",
        "print(\"Merged DataFrame with match status:\")\n",
        "display(merged_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48743e26"
      },
      "source": [
        "## Select frames for visualization\n",
        "\n",
        "### Subtask:\n",
        "Filter the merged DataFrame to get frames where at least one face was detected. Randomly select 10 unique frame indices from this filtered set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41e629dc"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the merged DataFrame to get frames where at least one face was detected, extract unique frame indices from the filtered set, and randomly sample up to 10 unique frame indices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43284a54"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Filter the merged_df to include only rows where 'match' is True\n",
        "matched_faces_in_merged_df = merged_df[merged_df['match'] == True]\n",
        "\n",
        "# Extract unique frame indices from the filtered DataFrame\n",
        "unique_matched_frame_indices = matched_faces_in_merged_df['frame_index'].unique()\n",
        "\n",
        "# Randomly sample up to 10 unique frame indices\n",
        "num_samples = min(10, len(unique_matched_frame_indices))\n",
        "selected_matched_frame_indices = np.random.choice(unique_matched_frame_indices, size=num_samples, replace=False)\n",
        "\n",
        "# Print the selected unique frame indices\n",
        "print(f\"Randomly selected {len(selected_matched_frame_indices)} unique matched frame indices:\")\n",
        "print(selected_matched_frame_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d26e1d22"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the selected unique matched frame indices, reload each frame from the video, detect all faces, and draw bounding boxes in red or green based on the match status of each face using the information in the merged_df DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4c83301"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure video_file_name is defined (from previous cells)\n",
        "# Ensure merged_df is available (from previous cells)\n",
        "\n",
        "# Open the video file again\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    print(f\"\\nVisualizing {len(selected_matched_frame_indices)} randomly selected matched frames:\")\n",
        "\n",
        "    for frame_index in selected_matched_frame_indices:\n",
        "        # Seek to the specific frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "\n",
        "        # Read the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            # Convert BGR to RGB for displaying with matplotlib\n",
        "            frame_to_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Get all face data for the current frame from the merged_df\n",
        "            frame_faces_df = merged_df[merged_df['frame_index'] == frame_index]\n",
        "\n",
        "            # Draw bounding boxes for all detected faces in this frame\n",
        "            for index, face_info in frame_faces_df.iterrows():\n",
        "                bbox = [face_info['bbox_x1'], face_info['bbox_y1'], face_info['bbox_x2'], face_info['bbox_y2']]\n",
        "                is_match = face_info['match']\n",
        "\n",
        "                # Ensure coordinates are integers\n",
        "                bbox = [int(b) for b in bbox]\n",
        "\n",
        "                # Determine bounding box color based on match status\n",
        "                color = (0, 255, 0) if is_match else (255, 0, 0) # Green for match, Red for no match\n",
        "\n",
        "                # Draw the bounding box\n",
        "                cv2.rectangle(frame_to_display, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n",
        "\n",
        "            # Show the frame using matplotlib\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(frame_to_display)\n",
        "            plt.title(f\"Frame {frame_index} - Detected Faces (Green: Match, Red: No Match)\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: Could not read frame {frame_index}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2b73723"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A total of 548 faces were detected across the sampled frames at a stride of 2.\n",
        "*   The detected faces, along with their frame index, unique ID, and bounding box coordinates, were successfully stored in a CSV file named \"all\\_faces\\_report.csv\".\n",
        "*   The \"all\\_faces\\_report.csv\" and \"output\\_report.csv\" files were successfully loaded and merged based on frame index and bounding box coordinates, associating match status with each detected face.\n",
        "*   The merged data confirmed that faces not present in \"output\\_report.csv\" were correctly identified as non-matches.\n",
        "*   Ten unique frame indices where at least one face was detected as a match were randomly selected for visualization.\n",
        "*   The selected frames were loaded from the video, and bounding boxes were drawn around all detected faces, colored green for matched faces and red for non-matched faces.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The process successfully captured and reported all detected faces, providing a comprehensive dataset for further analysis beyond just the matched faces.\n",
        "*   The visualization step effectively highlights both matched and non-matched faces within the same frames, offering a clearer picture of the detection and matching performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64b5842c"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "# Ensure query_embedding and similarity_threshold are defined from previous cells\n",
        "# Assuming query_embedding is a torch tensor and similarity_threshold is a float\n",
        "\n",
        "# 1. Save the current query embedding and threshold value into a Python dictionary.\n",
        "# If query_embedding is on GPU, move it to CPU before saving\n",
        "query_embedding_cpu = embedding.cpu() if embedding.is_cuda else embedding\n",
        "\n",
        "model_data = {\n",
        "    \"query_embedding\": query_embedding_cpu,\n",
        "    \"threshold\": similarity_threshold\n",
        "}\n",
        "\n",
        "# 2. Use the pickle library to save this dictionary into a file named \"face_match_model.pkl\".\n",
        "output_filename = \"face_match_model.pkl\"\n",
        "with open(output_filename, 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "# 3. Print a confirmation message when saved.\n",
        "print(f\"Model data (query embedding and threshold) saved to {output_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4ef6624"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "# Define the filename\n",
        "model_filename = \"face_match_model.pkl\"\n",
        "\n",
        "# 1. Load the \"face_match_model.pkl\" file using pickle.\n",
        "try:\n",
        "    with open(model_filename, 'rb') as f:\n",
        "        model_data = pickle.load(f)\n",
        "\n",
        "    # 2. Retrieve the query_embedding and threshold from the dictionary.\n",
        "    loaded_query_embedding = model_data.get(\"query_embedding\")\n",
        "    loaded_threshold = model_data.get(\"threshold\")\n",
        "\n",
        "    # 3. Print the shape of the query_embedding and threshold to confirm successful load.\n",
        "    if loaded_query_embedding is not None:\n",
        "        print(f\"Shape of loaded query_embedding: {loaded_query_embedding.shape}\")\n",
        "    else:\n",
        "        print(\"query_embedding not found in the loaded model data.\")\n",
        "\n",
        "    if loaded_threshold is not None:\n",
        "        print(f\"Loaded threshold value: {loaded_threshold}\")\n",
        "    else:\n",
        "        print(\"threshold not found in the loaded model data.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{model_filename}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the model data: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jq557sIoY2Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7e5236"
      },
      "source": [
        "## Visualize all matched frames\n",
        "\n",
        "### Subtask:\n",
        "Filter the merged DataFrame to get frames where the 'match' status is True. Iterate through these frames, reload them from the video, draw bounding boxes around the matched faces (green), and display them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e098122"
      },
      "source": [
        "**Reasoning**:\n",
        "Filter the merged DataFrame to include only the rows where 'match' is True. Then, iterate through the unique frame indices in this filtered DataFrame. For each frame index, reload the frame from the video, get all face data for that frame from the merged DataFrame, draw green bounding boxes around the faces where 'match' is True, and display the frame using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8b9ca35"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure video_file_name is defined (from previous cells)\n",
        "# Ensure merged_df is available (from previous cells)\n",
        "\n",
        "# Filter the merged_df to include only rows where 'match' is True\n",
        "all_matched_faces_df = merged_df[merged_df['match'] == True]\n",
        "\n",
        "# Get unique frame indices where a match was found\n",
        "all_matched_frame_indices = all_matched_faces_df['frame_index'].unique()\n",
        "\n",
        "print(f\"Visualizing all {len(all_matched_frame_indices)} frames where a match was detected:\")\n",
        "\n",
        "# Open the video file again\n",
        "cap = cv2.VideoCapture(video_file_name)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_file_name}\")\n",
        "else:\n",
        "    for frame_index in all_matched_frame_indices:\n",
        "        # Seek to the specific frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "\n",
        "        # Read the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            # Convert BGR to RGB for displaying with matplotlib\n",
        "            frame_to_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Get all face data for the current frame from the merged_df\n",
        "            frame_faces_df = merged_df[merged_df['frame_index'] == frame_index]\n",
        "\n",
        "            # Draw bounding boxes for all detected faces in this frame\n",
        "            for index, face_info in frame_faces_df.iterrows():\n",
        "                bbox = [face_info['bbox_x1'], face_info['bbox_y1'], face_info['bbox_x2'], face_info['bbox_y2']]\n",
        "                is_match = face_info['match']\n",
        "\n",
        "                # Ensure coordinates are integers\n",
        "                bbox = [int(b) for b in bbox]\n",
        "\n",
        "                # Draw bounding box only if it's a matched face (color green)\n",
        "                if is_match:\n",
        "                    color = (0, 255, 0) # Green for match\n",
        "                    cv2.rectangle(frame_to_display, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n",
        "\n",
        "            # Show the frame using matplotlib\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(frame_to_display)\n",
        "            plt.title(f\"Frame {frame_index} - Matched Face(s) Detected\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Warning: Could not read frame {frame_index}\")\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5691973"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The code successfully filtered the `merged_df` to identify all frames where at least one face matched the query embedding.\n",
        "* A total of {len(all_matched_frame_indices)} unique frames were identified as containing a match.\n",
        "* Each of these matched frames was reloaded from the video.\n",
        "* For each matched frame, bounding boxes were drawn around the faces that were identified as a match (in green).\n",
        "* The frames with the drawn bounding boxes were displayed using matplotlib, allowing for visual verification of the matching results.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* This visualization provides a comprehensive view of all instances where the query person was detected in the video based on the defined similarity threshold.\n",
        "* Further analysis could involve examining the distribution of these matched frames throughout the video to understand the duration and frequency of the person's appearance.\n",
        "* You could extend this visualization to include similarity scores or face IDs next to the bounding boxes for more detailed analysis."
      ]
    }
  ]
}